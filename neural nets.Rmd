---
title: "Neural net"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r data}
set.seed(1000)
  mortg = sample(c(0,1),size=5000,replace=TRUE)                    #Mortgage or not
  gender = sample(c(0,1),size=5000,replace = TRUE)  #gender male = 1 female = 0
  numcred = round(runif(5000,0,3))                  #number of credit lines
  xbeta = -6 + 1.4*mortg +0.7*gender + 1.9*numcred    #x*beta
  p = 1/(1+exp(-xbeta))
  
  y = rbinom(n=5000,size=1,prob=p)
  
  trainmortg = mortg[1:4000]
  traingen   = gender[1:4000]
  traincred = numcred[1:4000]
  trainy    = y[1:4000]
  
  testmortg = mortg[4001:5000]
  testgen = gender[4001:5000]
  testcred = numcred[4001:5000]
  testy = y[4001:5000]
  

  
```



```{r logistic}
model = glm(trainy ~ trainmortg + traingen + traincred, family = "binomial")
  
coef = summary(model)$coefficients[,1]

fitin = coef[1] + testmortg*coef[2] + testgen*coef[3] + testcred*coef[4]
fitout = 1/(1+exp(-fitin))
library(pROC)
roc.info =  roc(testy,fitout,plot=TRUE,legacy.axes=TRUE)
roc.df = data.frame(tpp=roc.info$sensitivities*100,fpp=(1-roc.info$specificities)*100,thresholds=roc.info$thresholds)

roc.df[roc.df$tpp>60,]
#threshold approx 0.2266 and has an approximate 80.82% true positive rate and approx 21.23% false positive rate

```



```{r training neural net}
weights = rep(1,4)
bias = rep(0.1,2)
lr = 0.01   #the learning rate
dhdw1 = rep(0,4000)  #initialising vector for derivative of h (hidden output) with respect to first weight
dhdw2 = rep(0,4000)  #similarly for weight2 
dhdw3 = rep(0,4000)  #and weight 3
dhdb1 = rep(0,4000)  #derivatives of h with respect to first bias
hout = rep(0,4000)


j = 0
while(j<100){
hin = weights[1]*trainmortg + weights[2]*traingen + weights[3]*traincred + bias[1]

for(i in 1:4000){
  a = hin[i]
if(a>0){
hout[i] = a
dhdw1[i] = trainmortg[i]   #leaky ReLU function - f(x) = x for x>0 f(x) =0.01*x for x<=0
dhdw2[i] = traingen[i]
dhdw3[i] = traincred[i]
dhdb1[i] = 1
}else{
  hout[i] = 0.01*a
  dhdw1[i] = 0.01*trainmortg[i]
  dhdw2[i] = 0.01*traingen[i]
  dhdw3[i] = 0.01*traincred[i]
  dhdb1[i] = 0.01
}
}
yin = weights[4]*hout + bias[2]
yout = 1/(1+exp(-yin))

dcostdy = 2*sum(yout-trainy)               #the derivative of the cost function with respect to the output (using quadratic loss function)

func = exp(yin)/((1+exp(yin))^2)           #this function will be used repeatedly in the derivatves, so only need to define once

dydw4 = hout*func                          #partial derivative of output with respect to weight 4
dydb2 = func                               #partial derivative of output with respect to second bias
dydh = weights[4]*func                     #partial derivative of final output with hiden output


dydw1 = dydh*dhdw1                         #derivatives of y with respect to weights in first layer
dydw2 = dydh*dhdw2
dydw3 = dydh*dhdw3
dydb1 = dydh*dhdb1

dcostdw4 = dcostdy*dydw4                   #the derivative of the cost function with respect to weight 4
dcostdb2 = dcostdy*dydb2                   #the derivative of the cost function with respect to second bias
dcostdw1 = dcostdy*dydw1
dcostdw2 = dcostdy*dydw2
dcostdw3 = dcostdy*dydw3
dcostdb1 = dcostdy*dydb1

weights[4] = weights[4] - lr*mean(dcostdw4)  #this is the formula for grad descent, note we are taking the average difference of weight changes using all data
bias[2] =   bias[2] - lr*mean(dcostdb2)
weights[3] = weights[3] - lr*mean(dcostdw3)
weights[2] = weights[2] - lr*mean(dcostdw2)
weights[1] = weights[1] - lr*mean(dcostdw1)
bias[1] =    bias[1] - lr*mean(dcostdb1)

cost = sum((yout-trainy)^2)
j = j+1
}
weights
bias

```
```{r testing NN}

hin = weights[1]*testmortg + weights[2]*testgen + weights[3]*testcred + bias[1]
hout = rep(0,1000)
predict = rep(0,1000)
truepos = 0
falsepos = 0
a1 = 0

for(i in 1:1000){
  c = hin[i]
  if(c>0){
    hout[i] = c
  }else{
    hout[i] = 0.01*c
  }
}
yin = weights[4]*hout + bias[2]
yout = 1/(1+exp(-yin))
for(i in 1:1000){
  d = yout[i]
  if(d>0.1679){
    predict[i] = 1 #predict default
  }else{
    predict[i] = 0 #predict non default
  }
  e = predict[i]
  f = testy[i]
  if((e>=1)&(f>=1)){
    truepos = truepos+1
  }
  if((e>=1)&(f<=0)){
    falsepos = falsepos+1
  }
  if((e<=0)&(f<=0)){
    a1 = a1+1
  }
}

trueprate = truepos/sum(testy==1)   #true positive rate
falseprate = falsepos/sum(testy==0) #false positive rate
trueprate
falseprate

```


```
